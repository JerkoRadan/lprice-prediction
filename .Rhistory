training_subset =subset(clean3_knn,sample ==TRUE)
test_subset = subset(clean3_knn,sample ==FALSE)
#-------Prepare Test Data-----------------------------------
colnames(test_df)[12] <- "dkeyboard"
colnames(test_df)[1] <- "id"
glimpse(test_df)
sum(is.na(test_df))
aggr(x=test_df[,6:20])
clean_test <- test_df
clean_test$screen_surface <- mapvalues(clean_test$screen_surface,c("glossy","matte"),c("Glossy","Matte"))
clean_test_knn <- knnImputation(clean_test)
aggr(x=clean_test_knn)
clean_test_knn %>%
summarise_if(is.factor,nlevels)
#--------- Data not normalized ---------------
# Selecting only the features to use
maxPrice_Clean_Training_prev <- clean3_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, max_price)
maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training_prev <- clean3_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, min_price)
minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
#-------- Data normalization -------------------
index_Categ <- match(c("brand", "touchscreen", "dkeyboard", "os", "max_price", "min_price"), names(clean3_knn))
preProcValues <- preProcess(clean3_knn[-index_Categ], method = "range")
trainScaled <- predict(preProcValues, clean3_knn)
glimpse(trainScaled)
testScaled <- predict(preProcValues, clean_test)
glimpse(testScaled)
#------Repeated K-Fold Cross Validation (K = 5, repeats = 3)----------------
# Selecting only the features to use
maxPrice_Norm_Training_prev <- trainScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, max_price)
maxPrice_Norm_Training <- data.frame(model.matrix(~., data=maxPrice_Norm_Training_prev))
maxPrice_Norm_Training
minPrice_Norm_Training_prev <- trainScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, min_price)
minPrice_Norm_Training <- data.frame(model.matrix(~., data=minPrice_Norm_Training_prev))
minPrice_Norm_Training
# Training control definition
set.seed(123)
train.control <- trainControl(method = "repeatedcv",
number = 5, repeats = 3)
#--------Models for maxPrice with Normalized data (except decision tree models) -----------------
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 1 (Linear regression)
model1_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
Test_prev <- clean_test_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os)
Price_Test <- data.frame(model.matrix(~., data=Test_prev))
glimpse(Test_prev)
glimpse(Price_Test)
model.matrix(~., data=Test_prev)
# Test data normalized
NormTest_prev <- clean_test_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os)
Price_NormTest <- data.frame(model.matrix(~., data=NormTest_prev))
#Adding missing columns
missingcol <- names(maxPrice_Clean_Training[!(names(maxPrice_Clean_Training[, !(names(maxPrice_Clean_Training) == "max_price")]) %in% names(Price_Test))])
Price_Test[missingcol] <- 0
Price_NormTest[missingcol] <- 0
predict(model2_min, Price_NormTest, type = "raw")
predict(model2_min, Price_NormTest, type = "response")
library(plyr)
library(tidyverse)
library(naniar)
library(VIM)
library(DMwR)
library(caret)
library(PerformanceAnalytics)
#--------Load Data-------------------------------------------
train_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/train.csv", na.strings = c("", "NA"))
test_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/test.csv", na.strings=c("NA",""))
#--------Load CPU and GPU Data-----------------------------------------
gpu_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/GPU_Benchmark.csv", na.strings = c("", "NA"))
cpu_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/CPU_Benchmark.csv", na.strings = c("", "NA"))
colnames(cpu_df)[1] <- "cpu_model"
colnames(cpu_df)[2] <- "cpu_benchmark_score"
colnames(gpu_df)[1] <- "gpu_model"
colnames(gpu_df)[2] <- "gpu_benchmark_score"
#--------Prepare Train Data---------------------------------
head(train_df)
sum(is.na(train_df))
colnames(train_df)[12] <- "dkeyboard"
colnames(train_df)[1] <- "id"
vis_miss(train_df,cluster= TRUE)
gg_miss_var(train_df)
gg_miss_case(train_df)
rown_four_nulls <- as.integer(rownames(train_df[rowSums(is.na(train_df[])) == 4,]))
clean2 <- train_df[-c(rown_four_nulls),]
gg_miss_var(clean2)
gg_miss_case(clean2)
clean2$screen_surface <- mapvalues(clean2$screen_surface,c("glossy", "matte"), c("Glossy", "Matte"))
aggr(x = clean2[,8:20])
glimpse(clean2)
clean3_knn <- knnImputation(clean2)
aggr(x=clean3_knn)
clean3_knn %>%
summarise_if(is.factor,nlevels)
clean4 <- clean3_knn %>%
mutate(resolution = pixels_x * pixels_y)
df_res <- unique(clean4[c("screen_size","pixels_x","pixels_y","resolution")])
df_res %>%
arrange(desc(resolution))
ggplot(clean4,aes(x=resolution,y=max_price,color=screen_size)) +
geom_point() +
scale_color_gradient(low="blue", high="red")
cor(clean4$resolution,clean4$max_price)
cor(clean4$resolution,clean4$max_price,method = "spearman")
cor(clean4$screen_size,clean4$max_price)
sort(unique(clean4$screen_size))
clean4 %>%
select(screen_size) %>%
table()
clean4[clean4$pixels_x == 3840, c("brand","base_name","screen_size","pixels_x","pixels_y")]
#--------------CPU Scores-----------------------------------------------
clean4<-clean4 %>%
mutate(cpu_details,cpu_clean= gsub("\\s*(\\d[.]\\d*)\\s*(GHz|ghz|Ghz|Ghz|gHz).*","",clean4$cpu_details))
cpu_df<-cpu_df %>%
mutate(cpu_model,cpu_clean= gsub("\\s*([@]).*|\\s*(APU).*","",cpu_df$cpu_model))
clean5 <- clean4 %>%
left_join(cpu_df,by="cpu_clean")
clean5$cpu_model <- as.character(clean5$cpu_model)
clean5$cpu_benchmark_score[is.na(clean5$cpu_benchmark_score)] <- 500
clean5$cpu_model[is.na(clean5$cpu_model)] <- "other"
#--------------GPU Scores-----------------------------------------------
clean6 <- mutate(clean5, gpu = ifelse(discrete_gpu == 0, 0,as.character(gpu)))
clean6<-clean6 %>%
mutate(gpu,gpu_model= gsub("^(\\S+\\s+\\n?){1}","",clean6$gpu))
gpu_df[,1] <- gsub(" with", "", gpu_df$gpu_model)
gpu_df[,1] <- gsub(" Design", "", gpu_df$gpu_model)
clean6$gpu_model <- gsub("GeFoce", "GeForce", clean6$gpu_model)
clean6$gpu_model <- gsub("GTX1070", "GTX 1070", clean6$gpu_model)
clean6 <- clean6 %>%
left_join(gpu_df,by="gpu_model")
clean6$gpu_benchmark_score[clean6$gpu_model == 0] <- 0
geforce_df <- filter(clean6, grepl('GeForce',clean6$gpu))
geforce_mean_score <- mean(geforce_df$gpu_benchmark_score, na.rm =TRUE)
clean6[is.na(clean6$gpu_benchmark_score) & grepl("GeForce",clean6$gpu_model),"gpu_benchmark_score"] <- geforce_mean_score
gpu_null <- clean6 %>%
select(gpu_model,gpu_benchmark_score) %>%
filter(is.na(clean6$gpu_benchmark_score))
clean6[is.na(clean6$gpu_benchmark_score),"gpu_benchmark_score"] <- mean(clean6$gpu_benchmark_score,na.rm=TRUE)
#-------Split Train Data to train/test subsets (80/20 percent) ----------------------
require(caTools)
set.seed(741)
sample = sample.split(clean3_knn$id,SplitRatio = 0.8)
training_subset =subset(clean3_knn,sample ==TRUE)
test_subset = subset(clean3_knn,sample ==FALSE)
#-------Prepare Test Data-----------------------------------
colnames(test_df)[12] <- "dkeyboard"
colnames(test_df)[1] <- "id"
glimpse(test_df)
sum(is.na(test_df))
aggr(x=test_df[,6:20])
clean_test <- test_df
clean_test$screen_surface <- mapvalues(clean_test$screen_surface,c("glossy","matte"),c("Glossy","Matte"))
clean_test_knn <- knnImputation(clean_test)
aggr(x=clean_test_knn)
clean_test_knn %>%
summarise_if(is.factor,nlevels)
#--------- Data not normalized ---------------
# Selecting only the features to use
maxPrice_Clean_Training_prev <- clean3_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, max_price)
maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training_prev <- clean3_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, min_price)
minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
# Adding pixels_x, discrete_gpu, removing os
maxPrice_Clean_Training_prev2 <- clean3_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, pixels_x, discrete_gpu, max_price)
maxPrice_Clean_Training2 <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev2))
minPrice_Clean_Training_prev2 <- clean3_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, pixels_x, discrete_gpu, min_price)
minPrice_Clean_Training2 <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev2))
# Adding pixels_x*pixels_y, discrete_gpu, removing os
clean3_knn$pixels_xy = clean3_knn$pixels_x*clean3_knn$pixels_y
maxPrice_Clean_Training_prev3 <- clean3_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, pixels_xy, discrete_gpu, max_price)
maxPrice_Clean_Training3 <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev3))
minPrice_Clean_Training_prev3 <- clean3_knn %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, pixels_xy, discrete_gpu, min_price)
minPrice_Clean_Training3 <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev3))
# Adding pixels_x*pixels_y, discrete_gpu, gpu_benchmark_score, cpu_benchmark_score, removing os ,removing dkeyboard
clean6$pixels_xy = clean6$pixels_x*clean6$pixels_y
maxPrice_Clean_Training_prev4 <- clean6 %>% select(brand, touchscreen, screen_size , weight, ram, storage, ssd, pixels_xy, discrete_gpu,cpu_benchmark_score,gpu_benchmark_score, max_price)
maxPrice_Clean_Training4 <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev4))
minPrice_Clean_Training_prev4 <- clean6 %>% select(brand, touchscreen, screen_size , weight, ram, storage, ssd, pixels_xy, discrete_gpu,cpu_benchmark_score,gpu_benchmark_score, min_price)
minPrice_Clean_Training4 <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev4))
# Training control definition
set.seed(123)
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
##### Train the model 11 Parallel Random Forest: pixels_x*pixels_y, discrete_gpu, gpu_benchmark_score, cpu_benchmark_score, removing os ,removing dkeyboard
model11_max <- train(max_price ~ . , data = maxPrice_Clean_Training4,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 11 Parallel Random Forest: pixels_x*pixels_y, discrete_gpu, gpu_benchmark_score, cpu_benchmark_score, removing os ,removing dkeyboard
model11_min <- train(min_price ~ . , data = minPrice_Clean_Training4,
method = "parRF", trControl = train.control, metric = "MAE")
View(clean6)
View(clean6)
View(clean_test_knn)
print(min(model11_max$results$MAE+model11_min$results$MAE)) # Adding pixels_x*pixels_y, discrete_gpu, gpu_benchmark_score,
# cpu_benchmark_score, removing os ,removing dkeyboard
ggplot(clean4,aes(x=resolution,y=max_price,color=screen_size)) +
geom_point() +
scale_color_gradient(low="blue", high="red")
cor(clean4$resolution,clean4$max_price)
cor(clean4$resolution,clean4$max_price,method = "spearman")
cor(clean4$screen_size,clean4$max_price)
sort(unique(clean4$screen_size))
clean4 %>%
select(screen_size) %>%
table()
clean4[clean4$pixels_x == 3840, c("brand","base_name","screen_size","pixels_x","pixels_y")]
clean_test1 <- clean_test_knn %>%
mutate(resolution = pixels_x * pixels_y)
#--------------CPU Scores for test data -----------------------------------------
clean_test1 <-clean_test1 %>%
mutate(cpu_details,cpu_clean= gsub("\\s*(\\d[.]\\d*)\\s*(GHz|ghz|Ghz|Ghz|gHz).*","",clean_test1$cpu_details))
cpu_df<-cpu_df %>%
mutate(cpu_model,cpu_clean= gsub("\\s*([@]).*|\\s*(APU).*","",cpu_df$cpu_model))
clean_test2 <- clean_test1 %>%
left_join(cpu_df,by="cpu_clean")
clean_test2$cpu_model <- as.character(clean_test2$cpu_model)
clean_test2$cpu_benchmark_score[is.na(clean_test2$cpu_benchmark_score)] <- 500
clean_test2$cpu_model[is.na(clean_test2$cpu_model)] <- "other"
#--------------GPU Scores for test data -----------------------------------------------
clean_test3 <- mutate(clean_test2, gpu = ifelse(discrete_gpu == 0, 0,as.character(gpu)))
clean_test3<-clean_test3 %>%
mutate(gpu,gpu_model= gsub("^(\\S+\\s+\\n?){1}","",clean_test3$gpu))
gpu_df[,1] <- gsub(" with", "", gpu_df$gpu_model)
gpu_df[,1] <- gsub(" Design", "", gpu_df$gpu_model)
clean_test3$gpu_model <- gsub("GeFoce", "GeForce", clean_test3$gpu_model)
clean_test3$gpu_model <- gsub("GTX1070", "GTX 1070", clean_test3$gpu_model)
clean_test3 <- clean_test3 %>%
left_join(gpu_df,by="gpu_model")
clean_test3$gpu_benchmark_score[clean6$gpu_model == 0] <- 0
geforce_df <- filter(clean_test3, grepl('GeForce',clean_test3$gpu))
geforce_mean_score <- mean(geforce_df$gpu_benchmark_score, na.rm =TRUE)
clean_test3[is.na(clean_test3$gpu_benchmark_score) & grepl("GeForce",clean_test3$gpu_model),"gpu_benchmark_score"] <- geforce_mean_score
gpu_null <- clean_test3 %>%
select(gpu_model,gpu_benchmark_score) %>%
filter(is.na(clean_test3$gpu_benchmark_score))
clean_test3[is.na(clean_test3$gpu_benchmark_score),"gpu_benchmark_score"] <- mean(clean_test3$gpu_benchmark_score,na.rm=TRUE)
#--------------GPU Scores for test data -----------------------------------------------
clean_test3 <- mutate(clean_test2, gpu = ifelse(discrete_gpu == 0, 0,as.character(gpu)))
clean_test3<-clean_test3 %>%
mutate(gpu,gpu_model= gsub("^(\\S+\\s+\\n?){1}","",clean_test3$gpu))
gpu_df[,1] <- gsub(" with", "", gpu_df$gpu_model)
gpu_df[,1] <- gsub(" Design", "", gpu_df$gpu_model)
clean_test3$gpu_model <- gsub("GeFoce", "GeForce", clean_test3$gpu_model)
clean_test3$gpu_model <- gsub("GTX1070", "GTX 1070", clean_test3$gpu_model)
clean_test3 <- clean_test3 %>%
left_join(gpu_df,by="gpu_model")
clean_test3$gpu_benchmark_score[clean_test3$gpu_model == 0] <- 0
geforce_df <- filter(clean_test3, grepl('GeForce',clean_test3$gpu))
geforce_mean_score <- mean(geforce_df$gpu_benchmark_score, na.rm =TRUE)
clean_test3[is.na(clean_test3$gpu_benchmark_score) & grepl("GeForce",clean_test3$gpu_model),"gpu_benchmark_score"] <- geforce_mean_score
gpu_null <- clean_test3 %>%
select(gpu_model,gpu_benchmark_score) %>%
filter(is.na(clean_test3$gpu_benchmark_score))
clean_test3[is.na(clean_test3$gpu_benchmark_score),"gpu_benchmark_score"] <- mean(clean_test3$gpu_benchmark_score,na.rm=TRUE)
View(clean_test3)
#--------Load Data-------------------------------------------
train_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/train.csv", na.strings = c("", "NA"))
test_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/test.csv", na.strings=c("NA",""))
#--------Load CPU and GPU Data-----------------------------------------
gpu_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/GPU_Benchmark.csv", na.strings = c("", "NA"))
cpu_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/CPU_Benchmark.csv", na.strings = c("", "NA"))
colnames(cpu_df)[1] <- "cpu_model"
colnames(cpu_df)[2] <- "cpu_benchmark_score"
colnames(gpu_df)[1] <- "gpu_model"
colnames(gpu_df)[2] <- "gpu_benchmark_score"
#--------Prepare Train Data---------------------------------
head(train_df)
sum(is.na(train_df))
colnames(train_df)[12] <- "dkeyboard"
colnames(train_df)[1] <- "id"
vis_miss(train_df,cluster= TRUE)
gg_miss_var(train_df)
gg_miss_case(train_df)
rown_four_nulls <- as.integer(rownames(train_df[rowSums(is.na(train_df[])) == 4,]))
clean2 <- train_df[-c(rown_four_nulls),]
gg_miss_var(clean2)
gg_miss_case(clean2)
clean2$screen_surface <- mapvalues(clean2$screen_surface,c("glossy", "matte"), c("Glossy", "Matte"))
aggr(x = clean2[,8:20])
glimpse(clean2)
clean3_knn <- knnImputation(clean2)
aggr(x=clean3_knn)
clean3_knn %>%
summarise_if(is.factor,nlevels)
clean4 <- clean3_knn %>%
mutate(resolution = pixels_x * pixels_y)
df_res <- unique(clean4[c("screen_size","pixels_x","pixels_y","resolution")])
df_res %>%
arrange(desc(resolution))
ggplot(clean4,aes(x=resolution,y=max_price,color=screen_size)) +
geom_point() +
scale_color_gradient(low="blue", high="red")
cor(clean4$resolution,clean4$max_price)
cor(clean4$resolution,clean4$max_price,method = "spearman")
cor(clean4$screen_size,clean4$max_price)
sort(unique(clean4$screen_size))
clean4 %>%
select(screen_size) %>%
table()
clean4[clean4$pixels_x == 3840, c("brand","base_name","screen_size","pixels_x","pixels_y")]
#--------------CPU Scores-----------------------------------------------
clean4<-clean4 %>%
mutate(cpu_details,cpu_clean= gsub("\\s*(\\d[.]\\d*)\\s*(GHz|ghz|Ghz|Ghz|gHz).*","",clean4$cpu_details))
cpu_df<-cpu_df %>%
mutate(cpu_model,cpu_clean= gsub("\\s*([@]).*|\\s*(APU).*","",cpu_df$cpu_model))
clean5 <- clean4 %>%
left_join(cpu_df,by="cpu_clean")
clean5$cpu_model <- as.character(clean5$cpu_model)
clean5$cpu_benchmark_score[is.na(clean5$cpu_benchmark_score)] <- 500
clean5$cpu_model[is.na(clean5$cpu_model)] <- "other"
#--------------GPU Scores-----------------------------------------------
clean6 <- mutate(clean5, gpu = ifelse(discrete_gpu == 0, 0,as.character(gpu)))
clean6<-clean6 %>%
mutate(gpu,gpu_model= gsub("^(\\S+\\s+\\n?){1}","",clean6$gpu))
gpu_df[,1] <- gsub(" with", "", gpu_df$gpu_model)
gpu_df[,1] <- gsub(" Design", "", gpu_df$gpu_model)
clean6$gpu_model <- gsub("GeFoce", "GeForce", clean6$gpu_model)
clean6$gpu_model <- gsub("GTX1070", "GTX 1070", clean6$gpu_model)
clean6 <- clean6 %>%
left_join(gpu_df,by="gpu_model")
clean6$gpu_benchmark_score[clean6$gpu_model == 0] <- 0
geforce_df <- filter(clean6, grepl('GeForce',clean6$gpu))
geforce_mean_score <- mean(geforce_df$gpu_benchmark_score, na.rm =TRUE)
clean6[is.na(clean6$gpu_benchmark_score) & grepl("GeForce",clean6$gpu_model),"gpu_benchmark_score"] <- geforce_mean_score
gpu_null <- clean6 %>%
select(gpu_model,gpu_benchmark_score) %>%
filter(is.na(clean6$gpu_benchmark_score))
clean6[is.na(clean6$gpu_benchmark_score),"gpu_benchmark_score"] <- mean(clean6$gpu_benchmark_score,na.rm=TRUE)
#-------Prepare Test Data-----------------------------------
colnames(test_df)[12] <- "dkeyboard"
colnames(test_df)[1] <- "id"
glimpse(test_df)
sum(is.na(test_df))
aggr(x=test_df[,6:20])
clean_test <- test_df
clean_test$screen_surface <- mapvalues(clean_test$screen_surface,c("glossy","matte"),c("Glossy","Matte"))
clean_test_knn <- knnImputation(clean_test)
aggr(x=clean_test_knn)
clean_test_knn %>%
summarise_if(is.factor,nlevels)
clean_test1 <- clean_test_knn %>%
mutate(resolution = pixels_x * pixels_y)
#--------------CPU Scores for test data -----------------------------------------
clean_test1 <-clean_test1 %>%
mutate(cpu_details,cpu_clean= gsub("\\s*(\\d[.]\\d*)\\s*(GHz|ghz|Ghz|Ghz|gHz).*","",clean_test1$cpu_details))
cpu_df<-cpu_df %>%
mutate(cpu_model,cpu_clean= gsub("\\s*([@]).*|\\s*(APU).*","",cpu_df$cpu_model))
clean_test2 <- clean_test1 %>%
left_join(cpu_df,by="cpu_clean")
clean_test2$cpu_model <- as.character(clean_test2$cpu_model)
clean_test2$cpu_benchmark_score[is.na(clean_test2$cpu_benchmark_score)] <- 500
clean_test2$cpu_model[is.na(clean_test2$cpu_model)] <- "other"
#--------------GPU Scores for test data -----------------------------------------------
clean_test3 <- mutate(clean_test2, gpu = ifelse(discrete_gpu == 0, 0,as.character(gpu)))
clean_test3<-clean_test3 %>%
mutate(gpu,gpu_model= gsub("^(\\S+\\s+\\n?){1}","",clean_test3$gpu))
gpu_df[,1] <- gsub(" with", "", gpu_df$gpu_model)
gpu_df[,1] <- gsub(" Design", "", gpu_df$gpu_model)
clean_test3$gpu_model <- gsub("GeFoce", "GeForce", clean_test3$gpu_model)
clean_test3$gpu_model <- gsub("GTX1070", "GTX 1070", clean_test3$gpu_model)
clean_test3 <- clean_test3 %>%
left_join(gpu_df,by="gpu_model")
clean_test3$gpu_benchmark_score[clean_test3$gpu_model == 0] <- 0
geforce_df <- filter(clean_test3, grepl('GeForce',clean_test3$gpu))
geforce_mean_score <- mean(geforce_df$gpu_benchmark_score, na.rm =TRUE)
clean_test3[is.na(clean_test3$gpu_benchmark_score) & grepl("GeForce",clean_test3$gpu_model),"gpu_benchmark_score"] <- geforce_mean_score
gpu_null <- clean_test3 %>%
select(gpu_model,gpu_benchmark_score) %>%
filter(is.na(clean_test3$gpu_benchmark_score))
clean_test3[is.na(clean_test3$gpu_benchmark_score),"gpu_benchmark_score"] <- mean(clean_test3$gpu_benchmark_score,na.rm=TRUE)
#--------- Data not normalized ---------------
# Selecting only the features to use
#Features: brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution(pixels_x*pixels_y), discrete_gpu,
#          cpu_benchmark_score, gpu_benchmark_score
maxPrice_Clean_Training_prev <- clean6 %>% select(brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,cpu_benchmark_score,gpu_benchmark_score, max_price)
maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
minPrice_Clean_Training_prev <- clean6 %>% select(brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,cpu_benchmark_score,gpu_benchmark_score, min_price)
minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
#-------- Data normalization -------------------
index_Categ <- match(c("brand", "touchscreen", "os", "max_price", "min_price"), names(clean6))
preProcValues <- preProcess(clean6[-index_Categ], method = "range")
trainScaled <- predict(preProcValues, clean6)
glimpse(trainScaled)
testScaled <- predict(preProcValues, clean_test3)
glimpse(testScaled)
index_Categ <- match(c("max_price", "min_price"), names(clean6))
preProcValues <- preProcess(clean6[-index_Categ], method = "range")
trainScaled <- predict(preProcValues, clean6)
glimpse(trainScaled)
index_Response <- match(c("max_price", "min_price"), names(clean6))
preProcValues <- preProcess(clean6[-index_Response], method = "range")
trainScaled <- predict(preProcValues, clean6)
glimpse(trainScaled)
testScaled <- predict(preProcValues, clean_test3)
glimpse(testScaled)
#------Repeated K-Fold Cross Validation (K = 20, repeats = 3)----------------
# Selecting only the features to use for Normalized data
maxPrice_Norm_Training_prev <- trainScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,cpu_benchmark_score,gpu_benchmark_score, max_price)
maxPrice_Norm_Training <- data.frame(model.matrix(~., data=maxPrice_Norm_Training_prev))
maxPrice_Norm_Training
minPrice_Norm_Training_prev <- trainScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,cpu_benchmark_score,gpu_benchmark_score, min_price)
minPrice_Norm_Training <- data.frame(model.matrix(~., data=minPrice_Norm_Training_prev))
minPrice_Norm_Training
# Training control definition
set.seed(123)
train.control <- trainControl(method = "repeatedcv",
number = 20, repeats = 3)
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5 Boosted Tree
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest <---------------BEST MODEL SO FAR
model7_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting
model8_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
warnings()
##### Train the model 1 (Linear regression)
model1_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5 Boosted Tree
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE")
##### Train the model 6 eXtreme Gradient Boosting
model6_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE")
##### Train the model 7 Parallel Random Forest <---------------BEST MODEL SO FAR
model7_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 8 Stochastic Gradient Boosting
model8_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "gbm", trControl = train.control, metric = "MAE")
print(model1_max$results$MAE+model1_min$results$MAE)
print(model2_max$results$MAE+model2_min$results$MAE)
print(model3_max$results$MAE+model3_min$results$MAE)
print(min(model4_max$results$MAE+model4_min$results$MAE))
print(min(model5_max$results$MAE+model5_min$results$MAE))
print(min(model6_max$results$MAE+model6_min$results$MAE))
print(min(model7_max$results$MAE+model7_min$results$MAE)) # <---------------BEST MODEL SO FAR
print(min(model8_max$results$MAE+model8_min$results$MAE))
##### Train the model 9 Parallel Random Forest with normalized data
model9_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "parRF", trControl = train.control, metric = "MAE")
##### Train the model 9 Parallel Random Forest with normalized data
model9_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "parRF", trControl = train.control, metric = "MAE")
print(min(model9_max$results$MAE+model9_min$results$MAE))
# Test data not normalized
Test_prev <- clean_test3 %>% select(brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,cpu_benchmark_score,gpu_benchmark_score)
Price_Test <- data.frame(model.matrix(~., data=Test_prev))
glimpse(Test_prev)
glimpse(Price_Test)
model.matrix(~., data=Test_prev)
# Test data normalized
NormTest_prev <- testScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, ssd, resolution, discrete_gpu,cpu_benchmark_score,gpu_benchmark_score)
Price_NormTest <- data.frame(model.matrix(~., data=NormTest_prev))
View(Price_Test)
View(testScaled)
#Adding missing columns (use corresponding training set)
missingcol <- names(maxPrice_Clean_Training[!(names(maxPrice_Clean_Training[, !(names(maxPrice_Clean_Training) == "max_price")]) %in% names(Price_Test))])
Price_Test[missingcol] <- 0
Price_NormTest[missingcol] <- 0
id_test <- clean_test3 %>% select(id)
bothModels <- list(model7_min ,model7_max)
pred <- data.frame(predict(bothModels, Price_Test, type = "raw")) #Parallel Random Forest (best so far)
names(pred) <- c("MIN","MAX")
results <- cbind(id_test,pred)
results
write.csv(results, file = "Model 3(Parallel Random Forest).csv", row.names = F)
id_test <- clean_test3 %>% select(id)
bothModels <- list(model9_min ,model9_max)
pred <- data.frame(predict(bothModels, Price_NormTest, type = "raw")) #Parallel Random Forest (best so far)
names(pred) <- c("MIN","MAX")
results <- cbind(id_test,pred)
results
write.csv(results, file = "Model 4(Parallel Random Forest with Normalized data).csv", row.names = F)
model7_max$results$MAE
model7_min$results$MAE
model8_max
model8_max$results$MAE
model8_min$results$MAE
