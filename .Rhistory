model1_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "lm", preProcess = c("center", "scale"), trControl = train.control, metric = "MAE") #warning a lot of features
library(plyr)
library(tidyverse)
library(naniar)
library(VIM)
library(DMwR)
library(caret)
#--------Load Data-------------------------------------------
train_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/train.csv", na.strings = c("", "NA"))
test_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/test.csv", na.strings=c("NA",""))
#--------Prepare Train Data---------------------------------
head(train_df)
sum(is.na(train_df))
colnames(train_df)[12] <- "dkeyboard"
colnames(train_df)[1] <- "id"
vis_miss(train_df,cluster= TRUE)
gg_miss_var(train_df)
gg_miss_case(train_df)
rown_four_nulls <- as.integer(rownames(train_df[rowSums(is.na(train_df[])) == 4,]))
clean2 <- train_df[-c(rown_four_nulls),]
gg_miss_var(clean2)
gg_miss_case(clean2)
clean2$screen_surface <- mapvalues(clean2$screen_surface,c("glossy", "matte"), c("Glossy", "Matte"))
aggr(x = clean2[,8:20])
glimpse(clean2)
clean3_knn <- knnImputation(clean2)
aggr(x=clean3_knn)
clean3_knn %>%
summarise_if(is.factor,nlevels)
#-------Split Train Data to train/test subsets (80/20 percent) ----------------------
require(caTools)
set.seed(741)
sample = sample.split(clean3_knn$id,SplitRatio = 0.8)
training_subset =subset(clean3_knn,sample ==TRUE)
test_subset = subset(clean3_knn,sample ==FALSE)
#-------Prepare Test Data-----------------------------------
colnames(test_df)[12] <- "dkeyboard"
colnames(test_df)[1] <- "id"
glimpse(test_df)
sum(is.na(test_df))
aggr(x=test_df[,6:20])
clean_test <- test_df
clean_test$screen_surface <- mapvalues(clean_test$screen_surface,c("glossy","matte"),c("Glossy","Matte"))
#------Repeated K-Fold Cross Validation (K = 10, repeats = 3)----------------
# Selecting only the features to use
maxPrice_Clean_Training <- training_subset %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, max_price)
glimpse(maxPrice_Clean_Training)
minPrice_Clean_Training <- training_subset %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, min_price)
glimpse(minPrice_Clean_Training)
# Training control definition
set.seed(123)
train.control <- trainControl(method = "repeatedcv",
number = 10, repeats = 3)
#--------Models for maxPrice -----------------
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 5
model4_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
#--------Models for min_price -----------------
model1_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
#------- Summarize the results----------------
print((model1_max$results$MAE+model1_min$results$MAE)/2)
print((model2_max$results$MAE+model2_min$results$MAE)/2)
print((model3_max$results$MAE+model3_min$results$MAE)/2)
print(min((model4_max$results$MAE+model4_min$results$MAE)/2))
#In progress
model1_max2 <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "lm", preProcess = c("center", "scale"), trControl = train.control, metric = "MAE") #warning a lot of features
model1_min2 <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "lm", preProcess = c("center", "scale"), trControl = train.control, metric = "MAE") #warning a lot of features
print((model1_max2$results$MAE+model1_min2$results$MAE)/2)
prueba <- maxPrice_Clean_Training
head(model.matrix(max_price ~., data = prueba))
descrCor <- cor(prueba)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)])>0.999)
descrCor <- cor(prueba2)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)])>0.999)
prueba2 <- head(model.matrix(max_price ~., data = prueba))
descrCor <- cor(prueba2)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)])>0.999)
highCorr
nzv <- nearZeroVar(prueba2)
nzv
nzv <- nearZeroVar(prueba2,saveMetrics= TRUE)
nzv
data.frame(table(prueba2))
descrCor <- cor(prueba2)
highCorr <- sum(abs(descrCor[upper.tri(descrCor)])>0.75)
highCorr
prueba[,-brand]
heads(prueba)
glimpse(prueba)
prueba %>% select(-brand)
prueba %>% select(-brand, -os)
scaleVariables <- prueba %>% select(-brand, -os) %>% preProcess(method = c("center", "scale"))
scaleVariables
scaleVariables <- preProcess(prueba[,-brand, -os],method = c("center", "scale"))
glimpse(scaleVariables)
library(plyr)
library(tidyverse)
library(naniar)
library(VIM)
library(DMwR)
library(caret)
scaleVariables <- preProcess(prueba[,-brand, -os],method = c("center", "scale"))
glimpse(scaleVariables)
train_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/train.csv", na.strings = c("", "NA"))
test_df <- read.csv("https://raw.githubusercontent.com/behnouri/lprice-prediction/master/test.csv", na.strings=c("NA",""))
#--------Prepare Train Data---------------------------------
head(train_df)
sum(is.na(train_df))
colnames(train_df)[12] <- "dkeyboard"
colnames(train_df)[1] <- "id"
vis_miss(train_df,cluster= TRUE)
gg_miss_var(train_df)
gg_miss_case(train_df)
rown_four_nulls <- as.integer(rownames(train_df[rowSums(is.na(train_df[])) == 4,]))
clean2 <- train_df[-c(rown_four_nulls),]
gg_miss_var(clean2)
gg_miss_case(clean2)
clean2$screen_surface <- mapvalues(clean2$screen_surface,c("glossy", "matte"), c("Glossy", "Matte"))
aggr(x = clean2[,8:20])
glimpse(clean2)
clean3_knn <- knnImputation(clean2)
aggr(x=clean3_knn)
clean3_knn %>%
summarise_if(is.factor,nlevels)
#-------Split Train Data to train/test subsets (80/20 percent) ----------------------
require(caTools)
set.seed(741)
sample = sample.split(clean3_knn$id,SplitRatio = 0.8)
training_subset =subset(clean3_knn,sample ==TRUE)
test_subset = subset(clean3_knn,sample ==FALSE)
#-------Prepare Test Data-----------------------------------
colnames(test_df)[12] <- "dkeyboard"
colnames(test_df)[1] <- "id"
glimpse(test_df)
sum(is.na(test_df))
aggr(x=test_df[,6:20])
clean_test <- test_df
clean_test$screen_surface <- mapvalues(clean_test$screen_surface,c("glossy","matte"),c("Glossy","Matte"))
#------Repeated K-Fold Cross Validation (K = 10, repeats = 3)----------------
# Selecting only the features to use
maxPrice_Clean_Training <- training_subset %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, max_price)
glimpse(maxPrice_Clean_Training)
minPrice_Clean_Training <- training_subset %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, min_price)
glimpse(minPrice_Clean_Training)
prueba <- maxPrice_Clean_Training
scaleVariables <- preProcess(prueba[,-brand, -os],method = c("center", "scale"))
glimpse(scaleVariables)
scaleVariables <- preProcess(prueba[,-brand -os],method = c("center", "scale"))
glimpse(scaleVariables)
prueba %>% index(select(-brand, -os))
index_Categ <- which(prueba == "brand"| "os")
index_Categ
index_Categ <- which(prueba = "brand"| "os")
index_Categ
index_Categ <- which(prueba == c("brand","os"))
index_Categ
index_Categ <- match(c("brand","os"), names(prueba))
index_Categ
glimpse(prueba)
preProcValues <- preProcess(prubea[-index_Categ], method = c("center", "scale"))
preProcValues
preProcValues <- preProcess(prueba[-index_Categ], method = c("center", "scale"))
preProcValues
trainScaled <- predict(preProcValues, prueba)
glimpse(trainScaled)
glimpse(prueba)
index_Categ <- match(c("brand", "touchscreen", "dkeyboard", "os"), names(prueba))
preProcValues <- preProcess(prueba[-index_Categ], method = c("center", "scale"))
trainScaled <- predict(preProcValues, prueba)
glimpse(trainScaled)
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
model1_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
print((model1_max$results$MAE+model1_min$results$MAE)/2)
index_Categ <- match(c("brand", "touchscreen", "dkeyboard", "os"), names(maxPrice_Clean_Training))
preProcValues <- preProcess(maxPrice_Clean_Training[-index_Categ], method = c("center", "scale"))
maxTrainScaled <- predict(preProcValues, maxPrice_Clean_Training)
minTrainScaled <- predict(preProcValues, minPrice_Clean_Training)
glimpse(maxTrainScaled)
#testScaleded <- predict(preProcValues, test)
model1_max2 <- train(max_price ~ . , data = maxTrainScaled,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
model1_min2 <- train(min_price ~ . , data = minTrainScaled,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
print((model1_max2$results$MAE+model1_min2$results$MAE)/2)
index_Categ <- match(c("brand", "touchscreen", "dkeyboard", "os"), names(maxPrice_Clean_Training))
preProcValuesMax <- preProcess(maxPrice_Clean_Training[-index_Categ], method = c("center", "scale"))
preProcValuesMin <- preProcess(minPrice_Clean_Training[-index_Categ], method = c("center", "scale"))
maxTrainScaled <- predict(preProcValuesMax, maxPrice_Clean_Training)
minTrainScaled <- predict(preProcValuesMin, minPrice_Clean_Training)
glimpse(maxTrainScaled)
glimpse(minTrainScaled)
model1_max2 <- train(max_price ~ . , data = maxTrainScaled,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
model1_min2 <- train(min_price ~ . , data = minTrainScaled,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
print((model1_max2$results$MAE+model1_min2$results$MAE)/2)
model1_max2
index_CategMax <- match(c("brand", "touchscreen", "dkeyboard", "os", "max_price"), names(maxPrice_Clean_Training))
index_CategMin <- match(c("brand", "touchscreen", "dkeyboard", "os", "min_price"), names(minPrice_Clean_Training))
preProcValuesMax <- preProcess(maxPrice_Clean_Training[-index_CategMax], method = c("center", "scale"))
preProcValuesMin <- preProcess(minPrice_Clean_Training[-index_CategMin], method = c("center", "scale"))
maxTrainScaled <- predict(preProcValuesMax, maxPrice_Clean_Training)
minTrainScaled <- predict(preProcValuesMin, minPrice_Clean_Training)
glimpse(maxTrainScaled)
glimpse(minTrainScaled)
#testScaleded <- predict(preProcValues, test)
model1_max2 <- train(max_price ~ . , data = maxTrainScaled,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
model1_min2 <- train(min_price ~ . , data = minTrainScaled,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
model1_max2
print((model1_max2$results$MAE+model1_min2$results$MAE)/2)
model1_max
print((model1_max$results$MAE+model1_min$results$MAE)/2)
index_CategMax <- match(c("brand", "touchscreen", "dkeyboard", "os", "max_price"), names(maxPrice_Clean_Training))
index_CategMin <- match(c("brand", "touchscreen", "dkeyboard", "os", "min_price"), names(minPrice_Clean_Training))
preProcValuesMax <- preProcess(maxPrice_Clean_Training[-index_CategMax], method = "range")
preProcValuesMin <- preProcess(minPrice_Clean_Training[-index_CategMin], method = "range")
maxTrainScaled <- predict(preProcValuesMax, maxPrice_Clean_Training)
minTrainScaled <- predict(preProcValuesMin, minPrice_Clean_Training)
glimpse(maxTrainScaled)
glimpse(minTrainScaled)
model1_max2 <- train(max_price ~ . , data = maxTrainScaled,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
model1_min2 <- train(min_price ~ . , data = minTrainScaled,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
model1_max2
model1_max
print((model1_max2$results$MAE+model1_min2$results$MAE)/2)
print((model1_max$results$MAE+model1_min$results$MAE)/2)
print((model1_max2$results$MAE+model1_min2$results$MAE)/2)
##### Train the model 4 (Elastic net (glm))
model4_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
model4_max2 <- train(max_price ~ . , data = maxTrainScaled,
method = "glmnet", trControl = train.control, metric = "MAE")
model4_min2 <- train(min_price ~ . , data = minTrainScaled,
method = "glmnet", trControl = train.control, metric = "MAE")
print(min((model4_max$results$MAE+model4_min$results$MAE)/2))
print(min((model4_max2$results$MAE+model4_min2$results$MAE)/2))
index_Categ <- match(c("brand", "touchscreen", "dkeyboard", "os", "max_price", "min_price"), names(training_subset))
preProcValues <- preProcess(training_subset[-index_Categ], method = "range")
trainScaled <- predict(preProcValues, training_subset)
glimpse(trainScaled)
#testScaleded <- predict(preProcValues, test) #Should also normalized the test data based on the training data
# Selecting only the features to use
maxPrice_Norm_Training <- trainScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, max_price)
glimpse(maxPrice_Norm_Training)
minPrice_Norm_Training <- trainScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, min_price)
glimpse(minPrice_Norm_Training)
##### Train the model 1 (Linear regression)
model1Norm_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2Norm_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3Norm_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4Norm_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
#--------Models for min_price with Normalized data -----------------
model1Norm_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2Norm_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3Norm_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4Norm_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
#------- Summarize the results with Normalized data ----------------
print((model1_max$results$MAE+model1_min$results$MAE)/2)
print((model2_max$results$MAE+model2_min$results$MAE)/2)
print((model3_max$results$MAE+model3_min$results$MAE)/2)
print(min((model4_max$results$MAE+model4_min$results$MAE)/2))
print((model1Norm_max$results$MAE+model1Norm_min$results$MAE)/2)
print((model2Norm_max$results$MAE+model2Norm_min$results$MAE)/2)
print((model3Norm_max$results$MAE+model3Norm_min$results$MAE)/2)
print(min((model4Norm_max$results$MAE+model4Norm_min$results$MAE)/2))
##### Train the model 3 (GLM with Step AIC)
model3_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 3 (GLM with Step AIC)
model3_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
print((model3_max$results$MAE+model3_min$results$MAE)/2)
##### Train the model 5
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "ANFIS", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "ANFIS", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "ANFIS", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "ANFIS", trControl = train.control, metric = "MAE") #warning a lot of features
warnings()
model5_min
warning()
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bartMachine", trControl = train.control, metric = "MAE") #warning a lot of features
1
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bartMachine", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bartMachine", trControl = train.control, metric = "MAE") #warning a lot of features
library(bartMachine)
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bartMachine", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "bayesglm", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "brnn", trControl = train.control, metric = "MAE") #warning a lot of features
warnings()
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "gamboost", trControl = train.control, metric = "MAE") #warning a lot of features
warnings()
model.matrix(max_price~., data=maxPrice_Norm_Training)
glimpse(maxPrice_Norm_Training)
maxPrice_Norm_Training_prev <- trainScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, max_price)
maxPrice_Norm_Training <- model.matrix(max_price~., data=maxPrice_Norm_Training_prev)
glimpse(maxPrice_Norm_Training)
maxPrice_Norm_Training
minPrice_Norm_Training_prev <- trainScaled %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, min_price)
minPrice_Norm_Training <- model.matrix(min_price~., data=minPrice_Norm_Training_prev)
minPrice_Norm_Training
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "gamboost", trControl = train.control, metric = "MAE") #warning a lot of features
type(maxPrice_Norm_Training)
class(maxPrice_Norm_Training)
maxPrice_Norm_Training <- data.frame(model.matrix(max_price~., data=maxPrice_Norm_Training_prev))
class(maxPrice_Norm_Training)
minPrice_Norm_Training <- data.frame(model.matrix(min_price~., data=minPrice_Norm_Training_prev))
minPrice_Norm_Training
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "gamboost", trControl = train.control, metric = "MAE") #warning a lot of features
maxPrice_Norm_Training <- dummy_cols(maxPrice_Norm_Training_prev)
class(maxPrice_Norm_Training)
maxPrice_Norm_Training <- dummy_columns(maxPrice_Norm_Training_prev)
class(maxPrice_Norm_Training)
maxPrice_Norm_Training <- data.frame(model.matrix(~., data=maxPrice_Norm_Training_prev))
class(maxPrice_Norm_Training)
minPrice_Norm_Training <- data.frame(model.matrix(~., data=minPrice_Norm_Training_prev))
minPrice_Norm_Training
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "gamboost", trControl = train.control, metric = "MAE") #warning a lot of features
warnings()
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
warnings()
##### Train the model 1 (Linear regression)
model1_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
#--------Models for min_price with Normalized data (except decision tree models) -----------------
##### Train the model 1 (Linear regression)
model1_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 2 (Generalized Linear Model without func specified -> could be improved)
model2_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glm", trControl = train.control, metric = "MAE") #warning a lot of features
##### Train the model 3 (GLM with Step AIC)
model3_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmStepAIC", trControl = train.control, metric = "MAE")
##### Train the model 4 (Elastic net (glm))
model4_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "glmnet", trControl = train.control, metric = "MAE")
#------- Summarize the results with Normalized data ----------------
print((model1_max$results$MAE+model1_min$results$MAE)/2)
print((model2_max$results$MAE+model2_min$results$MAE)/2)
print((model3_max$results$MAE+model3_min$results$MAE)/2)
print(min((model4_max$results$MAE+model4_min$results$MAE)/2))
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "gamboost", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "gamboost", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "brnn", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "nnet", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Norm_Training,
method = "nnet", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
maxPrice_Clean_Training_prev <- training_subset %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, max_price)
maxPrice_Clean_Training <- data.frame(model.matrix(~., data=maxPrice_Clean_Training_prev))
maxPrice_Clean_Training
minPrice_Clean_Training_prev <- training_subset %>% select(brand, touchscreen, screen_size , weight, ram, storage, dkeyboard, ssd, os, min_price)
minPrice_Clean_Training <- data.frame(model.matrix(~., data=minPrice_Clean_Training_prev))
minPrice_Clean_Training
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "nnet", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "mxnet", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "gamboost", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "brnn", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "glmboost", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "glmboost", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "BstLm", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "BstLm", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "BstLm", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "bstTree", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
print((model5_max$results$MAE+model5_min$results$MAE)/2)
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "DENFIS", trControl = train.control, metric = "MAE") #warning a lot of features
warnings()
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "enet", trControl = train.control, metric = "MAE") #warning a lot of features
warnings()
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "randomGLM", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "randomGLM", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "randomGLM", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE") #warning a lot of features
model5_min <- train(min_price ~ . , data = minPrice_Clean_Training,
method = "randomGLM", trControl = train.control, metric = "MAE") #warning a lot of features
print((model5_max$results$MAE+model5_min$results$MAE)/2)
model5_max <- train(max_price ~ . , data = maxPrice_Clean_Training,
method = "xgbTree", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "gamLoess", trControl = train.control, metric = "MAE") #warning a lot of features
model5_max <- train(max_price ~ . , data = maxPrice_Norm_Training,
method = "lm", trControl = train.control, metric = "MAE") #warning a lot of features
